---
title: "Statistical Methods 1"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 4
    css: style.css
---

<style>
    body .main-container {
        max-width: 1800px;
    }
</style>

<!--- For HTML Only --->
`r if (!knitr:::is_latex_output()) '
$\\DeclareMathOperator*{\\argmin}{arg\\,min}$
$\\DeclareMathOperator*{\\argmax}{arg\\,max}$
$\\newcommand{\\var}{\\mathrm{Var}}$
'`

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{listings}
\usepackage{color}
\usepackage[T1]{fontenc}
\usepackage{fullpage}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{fullpage}
\usepackage{color}
\usepackage{bm}
\usepackage{algorithm2e}
\newcommand{\def}{\overset{\text{def}}{:=}}
\newcommand{\lop}{\mathcal{L}}
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}}
\newcommand{\norm}[1]{||#1||}
\newcommand{\vx}{\vect{x}}
\newcommand{\vb}{\vect{b}}
\newcommand{\vy}{\vect{y}}
\newcommand{\vz}{\vect{z}}
\newcommand{\ve}{\vect{e}}
\newcommand{\yhat}{\widehat{y}}
\newcommand{\xhat}{\widehat{x}}
\newcommand{\vc}{\vect{c}}
\newcommand{\vr}{\vect{r}}
\newcommand{\vphi}{\vect{\phi}}
\newcommand{\vf}{\vect{f}}
\newcommand{\vY}{\vect{Y}}
\newcommand{\vX}{\vect{X}}
\newcommand{\vw}{\vect{w}}
\newcommand{\thetahatis}{\widehat{\theta}^{(s)}_i}
\newcommand{\thetahat}[1]{\widehat{\theta}^{(#1)}_i}
\newcommand{\vm}{\vect{m}}
\newcommand{\redmath}[1]{\mathbin{\textcolor{red}{\vect{#1}}}}
\newcommand{\redtext}[1]{\textcolor{red}{\vect{#1}}}
\newcommand{\vzero}{\vect{0}}
\newcommand{\vt}{\vect{t}}
\newcommand{\linearpredictor}{\vx_i^T\vbeta}
\newcommand{\vmu}{\vect{\mu}}
\newcommand{\vnu}{\vect{\nu}}
\newcommand{\veta}{\vect{\eta}}
\newcommand{\vbeta}{\vect{\beta}}
\newcommand{\vepsilon}{\vect{\epsilon}}
\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\Ebb}{\mathbb{E}}
\newcommand{\vdelta}{\vect{\delta}}
\newcommand{\Rbb}{\mathbb{R}}
\newcommand{\vxi}{\vect{\xi}}
\newcommand{\vu}{\vect{u}}
\newcommand{\vW}{\vect{W}}
\newcommand{\vlambda}{\vect{\lambda}}
\newcommand{\dataset}{\mathcal{D}}
\newcommand{\vS}{\vect{S}}
\newcommand{\sample}{\vz^{(l)}}
\newcommand{\MYhref}[3][blue]{\href{#2}{\color{#1}{#3}}}
\newcommand{\sol}[1]{\vx^{(#1)}}
\newcommand{\qtext}[1]{\quad\quad \text{#1}}
\newcommand{\vtheta}{\vect{\theta}}
\newcommand{\bi}[1]{\textbf{\textit{#1}}}
\newcommand{\iid}{\overset{\text{i.i.d.}}{\sim}}
\newcommand{\uniform}{\mathcal{U}(0, 1)}
\newcommand{\qimplies}{\quad\Longrightarrow\quad}
\newcommand{\tp}{\tilde{p}}
\newcommand{\nul}{\Theta^{(0)}}
\newcommand{\alter}{\Theta^{(1)}}
\newcommand{\const}{\mathcal{Z}}
\newcommand{\tq}{\tilde{q}}
\newcommand{\vxhat}{\widehat{\vx}}
\newcommand{\tvx}{\widetilde{\vx}}
\newcommand{\tr}{\tilde{r}}
\newcommand{\like}{\mathcal{L}}
\newcommand{\kl}[2]{\text{KL}(#1\,\,||\,\,#2)}
\newcommand{\logit}[1]{\log\left(\frac{#1}{1-#1}\right)}
\newcommand{\elbo}[1]{\text{elbo}(#1)}
\newcommand{\eval}{\biggr\rvert}
\newcommand{\normal}{\mathcal{N}}
\newcommand{\class}{\mathcal{C}}
\newcommand{\infor}{\mathcal{I}}
\newcommand{\variance}{\text{Var}}
\newcommand{\vSigma}{\vect{\Sigma}}
\newcommand{\lp}[3]{
    \begin{equation*}
    \begin{alignat}{2}
    &\!\min       &\qquad& #1\\
    &\text{s.t.} &      & #2\\
    &                  &      & #3
    \end{alignat}
    \end{equation*}
}
\newcommand{\lpmax}[3]{
    \begin{equation*}
    \begin{alignat}{2}
    &\!\max       &\qquad& #1\\
    &\text{s.t.} &      & #2\\
    &                  &      & #3
    \end{alignat}
    \end{equation*}
}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Course Description
Repository containing summary of notes for the [Statistical Methods 1](https://www.bris.ac.uk/unit-programme-catalogue/UnitDetails.jsa;jsessionid=7912981ABC434B62AA99EB75652E7E8A?ayrCode=19%2F20&unitCode=MATHM0041) course in the [Computational Statistics and Data Science (COMPASS)](http://www.bris.ac.uk/maths/postgraduate/compass/) Center for Doctoral Training (CDT) PhD at the [University of Bristol](https://www.bristol.ac.uk).


## Regression Problems

### Problem Statement and Assumptions
Given a **training**  dataset 
$$D_0:= \left\{(\vx_i, y_i)\right\}_{i=1}^n$$
of $n$ pairs of inputs $$\vx_i\ = (x_{1i}, \ldots, x_{di})\in\Rbb^{d\times 1}$$ and outputs $y_i\in \Rbb$ we want to **predict** the output $\yhat$ for a new input $\vxhat$ by _inferring_ the **parameters** $\vw\in \Rbb^{(d+1)\times 1}$ of a  **prediction function** $f(\vx, \vw)$.

### Measuring Performance and Model Selection
We want $f(\vx, \vw)$ to perform well on **unseen testing data** $D_1$. In other words, we want our model to **generalize** well. We define the **error** on a dataset $D'$ given the inferred parameters $\vw^*$ as 

$$E(D', \vw^*):=\sum_{i\in D'} \left(y_i - f(\vx_i, \vw^*)\right)^2$$

This gives two **performance metrics**:

* _Training Error_ : $E(D_0, \vw^*)$
* _Testing Error_ : $E(D_1, \vw^*)$

We could split our dataset into training set $D_0$ and testing set $D_1$, however data is often scarce so we want to use as much of it as possible. To fully exploit our dataset we can use **Cross Validation**:

* Split the dataset into $s$ groups $D_1, \ldots, D_s$ called **folds**. We then call it **s-fold cross validation**. In the extreme case in which $s=n$, i.e. each group consists of one data point, we call it **leave-one-out** cross validation.
* Use $s-1$ groups to infer $\vw^*$. Test performance of $f(\vx, \vw^*)$ on the remaining unseen group, and calculate $E(D_i, \vw^{*^{(i)}})$. Repeat this until all groups have been used for testing exactly once.
* At the end, calculate the mean test error  $$E_{\text{cv}} = \frac{1}{s+1}\sum_{i=1}^s E(D_i, \vw^{*^{(i)}})$$

Cross validation can be used to choose between **competing models**. Thus, it is often used for **model selection** when a model has one or more **hyperparameters** that needs to be estimated. The main problems with Cross Validation are:

* Dataset needs to be IID, but this is often not the case. For instance in a time-dependent dataset.
* Computational complexity of Cross Validation algorithm increases by a factor of $s$ for each hyperparameter.

### Non-Bayesian Inference Methods
We adopt the following notation: each column of the matrix $X$ represents a single observation of all the explanatory variable features, and each row represents all observations of a single feature. 

$$X = 
\begin{pmatrix}
  \vx_1 & \vx_2 & \cdots & \vx_n \\
  1 & 1 & \cdots 1
\end{pmatrix}
=
\begin{pmatrix}
  x_{11} & x_{12} & \cdots & x_{1n} \\
  x_{21} & x_{22} & \cdots & x_{2n} \\
  \vdots & \vdots & \vdots & \vdots \\
  x_{d1} & x_{d2} & \cdots & x_{dn} \\
  1 & 1 & \cdots & 1 
\end{pmatrix}
\in \Rbb^{(b+1)\times n}
$$

The **response vector** is a _row_ vector containing all $n$ observations of the target variable.
$$\vy = (y_1, \ldots, y_n)\in \Rbb^{1\times n}$$
We call the function 

$$\phi:\Rbb^d\to\Rbb^b$$
a **feature transform** and will be used to make inference more flexible. Sometimes we will write $\phi(X)$, this is a short-hand notation for the following matrix
$$
\phi(X) = 
\begin{pmatrix}
  \phi(\vx_1) & \phi(\vx_2) & \cdots & \phi(\vx_n) \\
  1 & 1 & \cdots & 1
\end{pmatrix}
\in \Rbb^{(b+1)\times n}
$$
There are $2$ main non-bayesian inference methods that we look at:


* **Linear Least Squares**: Finds parameters $\vw_{\text{LS}}\in\Rbb^{(b+1)\times 1}$ that minimize the _sum of squares residuals_ 

  $$\vw_{\text{LS}}:= \argmin_{\vw} \sum_{i\in D_0} \left(y_i  - f(\phi(\vx_i), \vw)\right)^2 = \left(\phi(X)\phi(X)^\top\right)^{-1}\phi(X)\vy^\top$$
  where $f(\phi(\vx_i), \vw)$ is the _dot product_ between the $i^{\text{th}}$ column of $\phi(X)$ and $\vw$. It is important to understand that it's called _linear_ because it is linear in the _unknown parameters_ $\vw$, but can be _non-linear_ in the inputs if we apply a *non-linear* feature transform $\phi$. 

* **Maximum Likelihood Estimation**: Finds parameters $\vw_{\text{ML}}$ describing a joint probability density function for $\vy$ that has _maximum density_ values at our data points
  $$
  \vw_{\text{ML}} = \argmax_{\vw} \,\,\,p(\vy \mid \phi(\vx_1), \ldots, \phi(\vx_n), \vw, \sigma)
  $$
    Assuming $y_i$ are **IID** and **normally distributed**, Maximum likelihood and Least Squares lead to the same solution $\vw_{\text{LS}} = \vw_{\text{ML}}$. The advantage of Maximum Likelihood over Least Squares is that by finding $\sigma^2_{\text{ML}}$ we obtain a **measure of uncertainty** on our prediction.

### Issues with Non-Bayesian Inference Methods

* They provide **point estimates** for our predictions, but we want a **predictive distribution** instead.
* Using the feature transform $\phi$ in the model leads both inference methods to possible **overfitting**, i.e. learning the _noise_ rather than the _structure_ in the data, when the **complexity** of the model, governed by $b$, increases. Cross Validation can be used to select the output dimensionality $b$ of $\phi$ to combat overfitting, but dataset is often not IID. A better choice is **regularized least squares**:
  $$
  \vw_{\text{LS-R}}:= \argmin_{\vw} \sum_{i\in D_0} \left(y_i - f(\phi(\vx_i), \vw)\right)^2 + \lambda || \vw ||_2^2 = \left(\phi(X)\phi(X)^\top + \lambda I \right)^{-1} \phi(X)\vy^\top 
  $$
  Regularization combats overfitting because this is often due to large increase in parameters magnitude, and regularization puts a penalty on it. To choose $\lambda$ we can use **cross-validation** or a **probabilistic perspective**, as we'll see later. Notice that above the penalty is the $L^2$ norm but can be any $p-$norm.
* They incur in the **curse of dimensionality** because as we increase complexity $b$, we include more features, whose number can grow exponentially even just in the case of $\phi$ being a polynomial transform. This requires a dataset size that is **exponential** in $b$.

### Semi and Fully Bayesian Inference Methods
Here we assume $y_i$ are IID and normally distributed. Moreover, we assign a _normal prior_ to the parameters.

* **Maximum-A-Posteriori** (MAP): Finds parameter $\vw_{\text{MAP}}$ that maximizes the **posterior parameter distribution** given our data $D$:
  $$
  \vw_{\text{MAP}} := \argmax_{\vw} p(\vw\mid D) = \argmax_{\vw} \prod_{i\in D} N_{y_i}\left(f(\phi(\vx_i), \vw), \sigma^2 \right)N_{\vw}(\vect{0}, \sigma^2_{\vw}I) = \vw_{\text{LS-R}}\left(\frac{\sigma^2}{\sigma^2_{\vw}}\right)
  $$
* **Fully Bayesian**: Finds a **predictive distribution** for $\yhat$ given the new $\vxhat$
  $$
  p(\yhat\mid \vxhat, D) = \int p(\yhat\mid \vxhat, D)p(\vw\mid D)d\vw = N_{\yhat}\left(f\left(\phi(\vxhat), \vw_{\text{LS-R}}\left(\frac{\sigma^2}{\sigma^2_{\vw}}\right)\right), \sigma^2 + \phi(\vxhat)^\top \sigma^2\left(\phi(X)\phi(X)^\top + \frac{\sigma^2}{\sigma^2_\vw} I \right)^{-1}\phi(\vxhat)\right)
  $$
  which is a normal distribution whose mean is the prediction given by regularized least squares. 
  
## Classification
### Problem Statement and Optimal Bayes Classifier
Now we consider $y\in \left\{+1, -1\right\}$. We are looking for a **decision function** $f(\vx)$ whose level set $f(\vx) = 0$, called **decision boundary**, separates feature space into two regions:

* $R_{+}$ where $f(\vx)\geq 0$ when $\vx\in R_{+}$
* $R_{-}$ where $f(\vx)\leq 0$ when $\vx\in R_{-}$

The **optimal Bayes classifier** 
$$f(\vw) = p(\vw, y=+1) - p(\vx, y= -1)$$ minimizes the probability of making a mistake given the decision function $f$, that is it minimizes
$$p(\text{mistake} \mid f) = \int_{R_{+}} p(\vx, y=-1)d\vx + \int_{R_{-}} p(\vx, y=+1)d\vx$$
However, this classifier is **unpractical** because in reality

* We don't have access to $p(\vx, y)$
* Different mistakes bear different losses.

### Expected Loss Minimization 
Therefore, we introduce a **loss function** $L(y, y_0)$ which is described by a **loss matrix** whose entries give the loss of predicting $y_0$ when the actual value is $y$. Our goal then becomes to **minimize expected loss**
$$\yhat = \argmin_{y_0} \Ebb_{p(y\mid \vx, D)}\left[y \mid \vx\right]$$
where $p(y \mid \vx, D)$ can be found with either the Fully Bayesian approach, or we can find $\vw_{\text{ML}}$ or $\vw_{\text{MAP}}$ first, and then plug them in to obtain the distribution. 

Notice that in general there are two strategies to find $p(y \mid \vx, D)$:

* **Discriminative Approach**: Infer $p(y \mid \vx, D)$ directly using $D$. This method is best used when our only goal is **prediction**.
* **Generative Approach**: Infer 
  $$
  p(y \mid \vx, D) \propto p(\vx \mid y, D) p(y)
  $$
  using Bayes rule, where $p(y)$ is simply the proportion of $+$ and $-$ in $D$. This gives us a way of _generating_ new data points, but at an extra cost because $p(\vx \mid y , D)$ can be very high-dimensional.
  
Once we have $p(y \mid \vx, D)$ we can start taking decisions. Sometimes, we might be uncertain about our decision. In such cases, we can reject a data point $\vx$ when both probabilities are lower than a **threshold** $\theta$ 
$$\max\left\{ p(y = +1\mid \vx), \, p(y=-1\mid \vx)\right\} < \theta$$

### Decision Making in Regression Tasks
$$\yhat \approx 
\begin{cases}
\Ebb_{p(y\mid \vx, D)}\left[y \mid \vx\right] && \text{if } L=(y-y_0)^2 \\
\text{Predictive median of } p(y \mid \vx, D) && \text{if } L=|y-y_0|
\end{cases}$$











































